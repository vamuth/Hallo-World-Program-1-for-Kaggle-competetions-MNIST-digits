{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":151,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install torchvision","execution_count":152,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.8.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.19.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.7.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision) (7.2.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchvision) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->torchvision) (0.6)\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install torchsummary\n","execution_count":153,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statistics import mean\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torchsummary import summary\nfrom torchvision import models","execution_count":154,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    device='cpu'\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    device='cuda'\n    print('CUDA is available!  Training on GPU ...')","execution_count":155,"outputs":[{"output_type":"stream","text":"CUDA is available!  Training on GPU ...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## read raw data \ndf_train = pd.read_csv('../input/digit-recognizer/train.csv')\ndf_test= pd.read_csv('../input/digit-recognizer/test.csv')\nlabels = torch.tensor(df_train['label'].values.astype(np.float32)).long()","execution_count":156,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## put it into a data set\ndf_train= df_train.drop(['label'], axis=1)\n\n#normalize to an image\ntrain_data = torch.tensor(df_train.values.reshape((-1,28,28)).astype('float32'))/255\ntest_data = df_test.values.reshape((-1,28,28))\n\ntraining_dataset = torch.utils.data.TensorDataset(train_data, labels)\n\ntrain_size = int(0.8 *len(training_dataset))\nvalidation_size = len(training_dataset) - train_size\n\ntrain_dataset, validation_dataset = torch.utils.data.random_split(training_dataset, [train_size, validation_size])\n\nbatch_size = 1\n\ndataloaders = dict()\ndataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\ndataloaders['validation'] = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n","execution_count":157,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn import functional as F\n\nclass digitnet(nn.Module):\n    \n    input_size = [28, 28]\n    output_size = 10\n    input_channels = 1\n    channels_conv1 = 9\n    channels_conv2 = 18\n    kernel_conv1 = [3, 3]\n    kernel_conv2 = [3, 3]\n    pool_conv1 = [2, 2]\n    pool_conv2 = [1, 2]\n    fcl1_size = 50\n    \n    def __init__(self):\n        super(digitnet, self).__init__()\n        \n        # Define the convolutional layers\n        self.conv1 = nn.Conv2d(self.input_channels, self.channels_conv1, self.kernel_conv1)\n        self.conv2 = nn.Conv2d(self.channels_conv1, self.channels_conv2, self.kernel_conv2)\n        \n        # Calculate the convolutional layers output size (stride = 1)\n        c1 = np.array(self.input_size) - self.kernel_conv1 + 1\n        p1 = c1 // self.pool_conv1\n        c2 = p1 - self.kernel_conv2 + 1\n        p2 = c2 // self.pool_conv2\n        self.conv_out_size = int(p2[0] * p2[1] * self.channels_conv2)\n        \n        # Define the fully connected layers\n        self.fcl1 = nn.Linear(self.conv_out_size, self.fcl1_size)\n        self.fcl2 = nn.Linear(self.fcl1_size, self.output_size)\n        \n    def forward(self, x):\n        \n        # reshape x and input size of network\n        x= x.unsqueeze(1)\n        # Apply convolution 1 and pooling\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, self.pool_conv1)\n        \n        # Apply convolution 2 and pooling\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, self.pool_conv2)\n        \n        # Reshape x to one dimmension to use as input for the fully connected layers\n        x = x.view(-1, self.conv_out_size)\n        \n        # Fully connected layers\n        x = self.fcl1(x)\n        x = F.relu(x)\n        x = self.fcl2(x)\n        \n        ## Last layer as per problem\n        x = F.log_softmax(x)\n        return x\n        ","execution_count":158,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = digitnet().to(device)\nprint(model)","execution_count":159,"outputs":[{"output_type":"stream","text":"digitnet(\n  (conv1): Conv2d(1, 9, kernel_size=[3, 3], stride=(1, 1))\n  (conv2): Conv2d(9, 18, kernel_size=[3, 3], stride=(1, 1))\n  (fcl1): Linear(in_features=990, out_features=50, bias=True)\n  (fcl2): Linear(in_features=50, out_features=10, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.25\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), learning_rate)\n#optimizer = torch.optim.Adam(model.parameters(), learning_rate)","execution_count":160,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.autograd import Variable\n\ntrain_losses = []\nvalidation_losses = []\ntest_accuracies=[]\n\n\nepochs = 20\nfor epoch in range(epochs):\n    model.train()\n    batch_train_losses =[]\n    batch_val_losses = []\n    \n    for i, (observations, labels) in enumerate(dataloaders['train']):\n        \n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(observations.to(device))\n        \n        # Backward pass\n        loss= criterion(outputs, labels.to(device))\n        loss.backward()\n        \n        \n        # Optimize\n        optimizer.step()\n        \n        # Accumulate losses\n        batch_train_losses.append(loss.cpu().detach().numpy())\n        \n        \n    # Testing on validation data\n    model.eval()\n    \n    correct = 0\n    total = 0\n    \n    \n    for i,(observations, labels) in enumerate(dataloaders['validation']):\n        \n        # Forward pass\n        outputs = model(observations.to(device))\n        \n        # Backward pass\n        loss = criterion(outputs, labels.to(device))\n        \n        # Accumulate losses\n        batch_val_losses.append(loss.cpu().detach().numpy())\n        \n        #Calculate accuracy\n        _,predicted = torch.max(outputs.data, 1)\n        correct += (predicted.cpu() == labels).sum().item()\n        total += labels.size(0)\n        \n    #Accumulate train and validation loss/accuracy for the epoch\n    train_losses.append(np.mean(batch_train_losses))\n    validation_losses.append(np.mean(batch_val_losses))\n    \n    accuracy = 100 * correct/total\n    test_accuracies.append(accuracy)\n    \n    \n    print('Epoch {} - Training Loss : {:.4f}, Testing Loss: {:.4f}, Test accuracy: {:.2f}% '\\\n          .format(epoch+1, np.mean(batch_train_losses), np.mean(batch_val_losses), accuracy))\n        \n    \n        \nplt.plot(train_losses)\nplt.show()\nplt.plot(validation_losses)\n        \n        ","execution_count":161,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-161-031dff1d1c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}